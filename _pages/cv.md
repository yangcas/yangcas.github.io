---
permalink: /
title: "About me"
toc: true
---
<p>I am a Research Scientist at <a href="https://www.nokia.com/bell-labs/about/locations/cambridge-uk/" target="_blank">Nokia Bell Labs</a>, Cambridge, UK, specializing in physics-informed and data-driven multimodal sensing and machine learning.  
My research bridges physical modeling, signal processing, and intelligent computation, enabling systems that understand human physiology, environment, and interaction in real-world settings.</p>

<p>Previously, I was a Marie Curie Research Fellow at The University of Sheffield, UK, where I led an EU Horizon 2020 project on battery-free backscatter communications under the supervision of 
<a href="https://www.sheffield.ac.uk/eee/people/academic-staff/xiaoli-chu" target="_blank">Professor Xiaoli Chu</a>.</p>


<p>I received a Ph.D. in Communication and Information Systems from the University of Chinese Academy of Sciences (UCAS), China, in July 2019, supervised by 
<a href="https://faculty.sist.shanghaitech.edu.cn/faculty/yangyang/" target="_blank">Professor Yang Yang</a>. 
I also earned a M.Eng. in Software Engineering from Beihang University in 2016, 
and a B.Eng. in Microelectronics from Anhui University in 2012.</p>

<p>I am always seeking <strong>new opportunities and collaborations</strong> that allow me to explore and expand the boundaries of physics-informed and data-driven multimodal technologies. I welcome partnerships that bridge research, innovation, and real-world impact.</p>



<h1 id="projects"><i class="ion-ios-lightbulb"></i> Projects</h1>




<h2 id="multimodal"><i class="ion-ios-gear"></i> Multimodal Sensing and Human-Centered Machine Learning</h2>

We design AI-enhanced wearable and earable systems that fuse multimodal biosignals from diverse sensors—such as photoplethysmography (PPG), inertial measurement units (IMUs), acoustic and temperature sensors, and millimeter-wave (mmWave) radar—to interpret physiological and behavioral states.
Projects such as OmniBuds, EarFusion, SPATIUM, and mmHvital exemplify this direction, integrating embedded machine learning, signal-quality-aware fusion, and spatiotemporal context modeling to advance health perception and human–computer interaction.

{% include_relative projects/omnibuds.md %}

{% include_relative projects/mobicom25demo.md %}

{% include_relative projects/spatium.md %}

{% include_relative projects/earfusion.md %}

{% include_relative projects/mmHvital.md %}



<h2 id="wireless"><i class="ion-wifi"></i> Physics-Informed Signal Processing for Data-Driven Wireless Intelligence</h2>
<p>
I explore physics-informed and data-driven signal processing combined with neural architectures to advance speech, radar, and acoustic sensing.  
Projects such as LPCSE demonstrate how classical physical and rule-based models can be embedded within end-to-end data-driven machine learning frameworks, achieving efficient, interpretable, and physically grounded intelligence.  
Projects on auditory attention decoding and otoacoustic emission analysis build on physics-informed physiological and auditory principles to interpret received signals and extract informative features, enabling data-driven learning and inference.
</p>



{% include_relative projects/lpcse.md %}
{% include_relative projects/attention.md %}

{% include_relative projects/oae.md %}

{% include_relative projects/backscatter.md %}

{% include_relative projects/pamt.md %}

<!-- {% include_relative projects/lowpower.md %} -->


<h2 id="simulation"><i class="ion-wifi"></i> Multiphysics Simulation</h2>

{% include_relative projects/simulations.md %}



{% include_relative projects/publications.md %}