<h3 id="omnibuds-demo" short-label = "OmniBuds Demo"> OmniBuds Demo: AI-Powered Multimodal Sensing for Human Enhancement
</h3>

<a href="https://www.sigmobile.org/mobicom/2025/" style="color: blue; font-size: 0.9em;">[To be demonstrated at ACM MobiCom 2025, Hong Kong, China]</a>

In this demo, OmniBuds act as an *AI-enhanced human–machine interface* to control the Chrome T-Rex game. Head and body motions detected through IMU data trigger jumping and ducking actions.
A real-time visualization dashboard displays raw IMU and PPG waveforms, derived vital signs, and gameplay feedback. The AI assistant analyses user performance, which includes reaction time, movement quality, fatigue indicators, and heart rate and provides adaptive auditory cues such as “Jump/Duck now!” through the earbuds.

As shown in the right figure below, the system architecture integrates multimodal sensing, machine learning  based analysis, and personalized feedback loops. The AI layer fuses physiological and motion data from OmniBuds to enable adaptive, context-aware applications such as gaming and human–computer interaction.

This demo aims to demonstrate how multimodal learning and physiological inference can enhance human performance and engagement in interactive tasks.

<table style="text-align:center; margin:auto;">
<tr>

<td>
<a href="/assets/paper_img/T-REX/demo_setup.png">
    <img src="/assets/paper_img/T-REX/demo_setup.png" style="width:50vw; border-radius:8px;"/>
</a>
<div><b>Live Demo Setup</b><br><small>User interacts with the T-Rex game via head and body motion while real-time sensor data and vitals are visualized.</small></div>
</td>
<td>
<a href="/assets/paper_img/T-REX/demo_flow.png">
    <img src="/assets/paper_img/T-REX/demo_flow.png" style="width:50vw; border-radius:8px;"/>
</a>
<div><b>System Architecture</b><br><small>OmniBuds-based multimodal sensing and feedback framework integrating IMU, PPG, and physiological inference.</small></div>
</td>
</tr>
</table>
